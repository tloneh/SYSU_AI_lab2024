# **中山大学计算机学院**

**人工智能**

**本科生实验报告**

课程名称：Artificial Intelligence

| 学号 | 22336084 | 姓名 | 胡舸耀 |
| ---- | -------- | ---- | ------ |

## 一、实验题目

**信誉度分类任务**

`DT_data.csv`数据集包含三列共600条数据，其中每条数据有如下属性：

* Undergrad : person is under graduated or not
* MaritalStatus : marital status of a person
* TaxableIncome : Taxable income is the amount of how much tax an individual owes to the government
* WorkExperience : Work experience of an individual person
* Urban : Whether that person belongs to urban area or not

将那些 `TaxableIncome <= 30000` 的人视为”有风险”，而其他人则为“好”。利用决策树算法实现对个人信誉度的分类。

## 二、实验内容

### 1.算法原理

#### （1）加载数据集

```python
def load_dataset(filename):
    dataset = []
    with open(filename, 'r') as file:
        for line in file.readlines():
            row = line.strip().split(',')
            dataset.append(row)
    return dataset
```

用with open打开文件，读取内容并分割.

#### （2）数据预处理

```python
def preprocess_dataset(dataset):
    X = []  # 特征
    y = []  # 标签
    for row in dataset[1:]:  # 忽略数据集的首行
        # 将特征值转换为数值类型
        features = [1 if row[i] == 'YES' else 0 if row[i] == 'NO' else 0 if row[i] == 'Single' else 
                    1 if row[i] == 'Divorced' else 2 if row[i] == 'Married' else int(row[i]) for i in range(4)]
        X.append(features)  # 前四列为特征值
        # 标签为1表示“有风险”，0表示“好”
        label = 1 if int(row[2]) <= 30000 else 0  # 第三列为TaxableIncome
        y.append(label)  # 添加到标签列表中
    return X, y
```

其中将五个数据都作为特征，按要求将第三列，`TaxableIncome <= 30000` 的人视为”有风险”，而其他人则为“好“，即1和0作为标签。

”YES“等字符串转化为0，1，2……便于训练.

#### （3）数据分割

```python
def train_test_split(dataset, split_ratio=0.3, random_seed=None):
    if random_seed is not None:
        random.seed(random_seed)
    train_size = int(len(dataset) * split_ratio)
    train_set = []
    test_set = list(dataset)
    while len(train_set) < train_size:
        index = random.randrange(len(test_set))
        train_set.append(test_set.pop(index))
    return train_set, test_set
```

通过调整split_ratio与random_seed调整模型参数，提升模型性能，split_ratio作为分割率，random_seed做随机种子分割训练集与测试集。

#### （4）构建模型

这部分代码较长，主要放在代码展示部分。

```python
def build_tree(train, max_depth, min_size):
    root = get_split(train)
    split(root, max_depth, min_size, 1)
    return root
```

具有多个特征值，用基尼指数做划分选择，利用 `get_split(train) `选择最佳划分点，对于数据集中的每个样本，计算使用当前特征进行划分后的基尼指数，并更新最佳划分的相关变量，返回最佳划分的特征索引、切分点和划分后的数据集信息。

再利用 `split(node, max_depth, min_size, depth)`在最大深度以及最小节点数的限制下，迭代剪枝生成决策树。

#### （5）预测以及计算准确率

```python
# 使用决策树进行预测
def predict(node, row):
    if row[node['index']] < node['value']:
        if isinstance(node['left'], dict):
            return predict(node['left'], row)
        else:
            return node['left']
    else:
        if isinstance(node['right'], dict):
            return predict(node['right'], row)
        else:
            return node['right']


# 计算准确率
def accuracy_metric(actual, predicted):
    correct = 0
    for i in range(len(actual)):
        if actual[i] == predicted[i]:
            correct += 1
    return correct / float(len(actual)) * 1.0
```

预测结果，函数首先根据节点的划分条件（特征索引和切分点）将输入样本划分到左子树或右子树中，然后递归地对子树进行预测，直到到达叶子节点为止。并且将结果与实际标签进行比较计算决策树预测的准确率。

### 2.关键代码展示

```python
# 构建决策树模型
def build_tree(train, max_depth, min_size):
    root = get_split(train)
    split(root, max_depth, min_size, 1)
    return root

def get_split(dataset):
    class_values = list(set(row[-1] for row in dataset))
    b_index, b_value, b_score, b_groups = 999, 999, 999, None
    for index in range(len(dataset[0])-1):
        for row in dataset:
            groups = test_split(index, row[index], dataset)
            gini = gini_index(groups, class_values)
            if gini < b_score:
                b_index, b_value, b_score, b_groups = index, row[index], gini, groups
    return {'index':b_index, 'value':b_value, 'groups':b_groups}

def test_split(index, value, dataset):
    left, right = [], []
    for row in dataset:
        if row[index] < value:
            left.append(row)
        else:
            right.append(row)
    return left, right

def gini_index(groups, classes):
    n_instances = float(sum(len(group) for group in groups))
    gini = 0.0
    for group in groups:
        size = float(len(group))
        if size == 0:
            continue
        score = 0.0
        for class_val in classes:
            p = [row[-1] for row in group].count(class_val) / size
            score += p * p
        gini += (1.0 - score) * (size / n_instances)
    return gini

def to_terminal(group):
    outcomes = [row[-1] for row in group]
    return max(set(outcomes), key=outcomes.count)

def split(node, max_depth, min_size, depth):
    left, right = node['groups']
    del(node['groups'])
    # 检查是否没有分割
    if not left or not right:
        node['left'] = node['right'] = to_terminal(left + right)
        return
    # 检查最大深度
    if depth >= max_depth:
        node['left'], node['right'] = to_terminal(left), to_terminal(right)
        return
    # 左节点
    if len(left) <= min_size:
        node['left'] = to_terminal(left)
    else:
        node['left'] = get_split(left)
        split(node['left'], max_depth, min_size, depth+1)
    # 右节点
    if len(right) <= min_size:
        node['right'] = to_terminal(right)
    else:
        node['right'] = get_split(right)
```

主函数部分

```python
# 主函数
def main():
    # 加载数据集
    dataset = load_dataset('DT_data.csv')

    # 数据预处理
    X, y = preprocess_dataset(dataset)

    # 划分数据集为训练集和测试集
    train_set, test_set = train_test_split(list(zip(X, y)))

    # 构建决策树模型
    max_depth = 5
    min_size = 10
    tree = build_tree(train_set, max_depth, min_size)

    # 在测试集上进行预测
    actual = [row[-1] for row in test_set]
    predicted = [predict(tree, row[:-1]) for row in test_set]

    # 计算准确率
    accuracy = accuracy_metric(actual, predicted)
    print("Accuracy:", accuracy)
```

## 三、实验结果与分析

#### 1.实验结果分析与示例

首先分析出一个好的分割种子以及分割测试、训练数据比例

通过遍历0-100内随机种子，显示在0.3分割率下正确率高于0.90的结果:

![1716039971811](image/实验作业7/1716039971811.png)

可以看到在随机种子为86时，准确率最高，为0.926

当改变分割率时，遍历分割率为0.1-0.9步长0.1，可以看到测试集与训练集比为3：7时效果最好，准确率为0.926

![1716040418872](image/实验作业7/1716040418872.png)

综上调参步骤，得出最好参数，训练结果如下，准确率为0.926

![1716040667040](image/实验作业7/1716040667040.png)

决策树可视化部分需要sklearn高级库的tree.plot()函数，故在此不做可视化处理

#### 2.评价指标展示与分析

利用基尼指数

$$
Gini(D)=\sum^{n}_{k=1}\sum_{k'\neq k}{\alpha_{k}\alpha_{k'}}=1-\sum_{k=1}^{n}{\alpha_{k}^2}
$$

基尼指数就是在样本集中随机抽出两个样本不同类别的概率。当样本集越不纯的时候，这个概率也就越大，即基尼指数也越大。

```python
def gini_index(groups, classes):
    n_instances = float(sum(len(group) for group in groups))
    gini = 0.0
    for group in groups:
        size = float(len(group))
        if size == 0:
            continue
        score = 0.0
        for class_val in classes:
            p = [row[-1] for row in group].count(class_val) / size
            score += p * p
        gini += (1.0 - score) * (size / n_instances)
    return gini
```

## 四、参考资料

[决策树原理](https://blog.csdn.net/GreenYang5277/article/details/104500739)
